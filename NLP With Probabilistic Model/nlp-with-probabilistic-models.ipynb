{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with Probabilistic Models\n",
    "## Autocorrect and Minimum Edit Distance\n",
    "### Autocorrect\n",
    "`Autocorrect is an application that changes the mispelled words into correct ones. Key steps include:`\n",
    "* Identify a misspelled word\n",
    "    * If a word doesnt exist in dictionar, it is probably mis-spelled\n",
    "    * If a word is not in a dictionary, flag it for correction\n",
    "* Find string n-edits distance away\n",
    "    * An edit is type of operation performed on a string to change it inot another string\n",
    "    * An edit distance counts the number of these operations\n",
    "    * n-distance telss how many operations away one string is from another\n",
    "    * Includes\n",
    "        * Insert (add a letter)\n",
    "        * Delete (remove a letter)\n",
    "        * Switch (exchange places)\n",
    "        * Replace\n",
    "    * If string is one edit distance away from the string typed, it is more similar to string compared to a string that is two edit distance away\n",
    "* Filter the strings for real words that are spelled correctly\n",
    "    * Strings generated do not look like actual words. We may want to consider real and correctly spelled words so we compare it to a known dictionary\n",
    "* Calculate word probabilities\n",
    "    * Haivng a list of words, we can calculate word probabilites of each word in the corpous (corpous is body of text)\n",
    "    * First calculate frequencies\n",
    "        * Calculate the total number of words in the body of text or corpous\n",
    "        * Probability of any word within the corpus is the number of times the word appears divided by total number of words\n",
    "        \n",
    "Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.\n",
    "\n",
    "$$P(w_i) = \\frac{C(w_i)}{M} \\tag{Eqn-2}$$\n",
    "where \n",
    "\n",
    "$C(w_i)$ is the total number of times $w_i$ appears in the corpus.\n",
    "\n",
    "$M$ is the total number of words in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all the libraries and modules used in the course\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re # regular expression library; for tokenization of words\n",
    "from collections import Counter # collections library; counter: dict subclass for counting hashable objects\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import nltk               # NLP toolkit\n",
    "import re                 # Library for Regular expression operations\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "import emoji\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from typing import Set, Dict, List\n",
    "\n",
    "nltk.download('punkt')    # Download the Punkt sentence tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am currently enrolled in NATURAL LANGUAGE PROCESSING Specialisatoin and I am really happy with SPECIALISATION\n",
      "string length :  111\n",
      "['i', 'am', 'currently', 'enrolled', 'in', 'natural', 'language', 'processing', 'specialisatoin', 'and', 'i', 'am', 'really', 'happy', 'with', 'specialisation']\n",
      "Count:  16\n"
     ]
    }
   ],
   "source": [
    "# the tiny corpus of text !\n",
    "text : str = \"I am currently enrolled in NATURAL LANGUAGE PROCESSING Specialisatoin and I am really happy with SPECIALISATION\"  # ðŸŒˆ\n",
    "print(text)\n",
    "print(\"string length : \", len(text))\n",
    "\n",
    "# preprocessing\n",
    "\n",
    "text_lowercase : str = text.lower() #converting the text to lowercase\n",
    "words : str = re.findall(r'\\w+', text_lowercase)\n",
    "print(words)\n",
    "print(\"Count: \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural', 'language', 'and', 'enrolled', 'happy', 'am', 'specialisatoin', 'with', 'currently', 'processing', 'i', 'in', 'really', 'specialisation'}\n",
      "Count:  14\n"
     ]
    }
   ],
   "source": [
    "# creating vocabulary\n",
    "\n",
    "vocab : Set = set(words) #using set to remove all the repetitive words\n",
    "print(vocab)\n",
    "print(f\"Count: \", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 2, 'am': 2, 'currently': 1, 'enrolled': 1, 'in': 1, 'natural': 1, 'language': 1, 'processing': 1, 'specialisatoin': 1, 'and': 1, 'really': 1, 'happy': 1, 'with': 1, 'specialisation': 1})\n",
      "Count:  14\n"
     ]
    }
   ],
   "source": [
    "# creating vocab including word count using collections.Counter\n",
    "\n",
    "counts_b: Dict[str, int] = dict()\n",
    "counts_b = Counter(words)\n",
    "print(counts_b)\n",
    "print(f\"Count: \", len(counts_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bar chart for sorted word counts\n",
    "d: Dict = {\n",
    "    \"i\": counts_b[\"i\"],\n",
    "    \"am\": counts_b[\"am\"],\n",
    "    \"currently\": counts_b[\"currently\"],\n",
    "    \"enrolled\": counts_b[\"enrolled\"],\n",
    "    \"happy\": counts_b[\"happy\"],\n",
    "}\n",
    "plt.bar(range(len(d)), list(d.values()), align=\"center\")\n",
    "_ = plt.xticks(range(len(d)), list(d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_b :  Counter({'hello': 5, 'i': 2, 'am': 2, 'new_word': 2, 'currently': 1, 'enrolled': 1, 'in': 1, 'natural': 1, 'language': 1, 'processing': 1, 'specialisatoin': 1, 'and': 1, 'really': 1, 'happy': 1, 'with': 1, 'specialisation': 1})\n",
      "count :  16\n"
     ]
    }
   ],
   "source": [
    "counts_b[\"hello\"] = 5\n",
    "print(\"counts_b : \", counts_b)\n",
    "print(\"count : \", len(counts_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'hallo']\n",
      "['h', 'allo']\n",
      "['ha', 'llo']\n",
      "['hal', 'lo']\n",
      "['hall', 'o']\n",
      "['hallo', '']\n"
     ]
    }
   ],
   "source": [
    "word = 'hallo'\n",
    "# splitting the word with a loop\n",
    "splits_a : List[str] = []\n",
    "for i in range(len(word)+1):\n",
    "    splits_a.append([word[:i], word[i:]])\n",
    "    \n",
    "for i in splits_a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 'hallo')\n",
      "('h', 'allo')\n",
      "('ha', 'llo')\n",
      "('hal', 'lo')\n",
      "('hall', 'o')\n",
      "('hallo', '')\n"
     ]
    }
   ],
   "source": [
    "# the same split can be done using a list comprehension\n",
    "\n",
    "splits_b : List[str] = [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "\n",
    "for i in splits_b:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allo\n",
      "allo  <-- delete  h\n",
      "hllo\n",
      "hllo  <-- delete  a\n",
      "halo\n",
      "halo  <-- delete  l\n",
      "halo\n",
      "halo  <-- delete  l\n",
      "hall\n",
      "hall  <-- delete  o\n"
     ]
    }
   ],
   "source": [
    "# delete a word from each string in the splits list\n",
    "\n",
    "splits = splits_a\n",
    "deletes : List[str] = []\n",
    "\n",
    "for L, R in splits:\n",
    "    if R:\n",
    "        print(L + R[1:])\n",
    "        print(L + R[1:], \" <-- delete \", R[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allo', 'hllo', 'halo', 'halo', 'hall']\n"
     ]
    }
   ],
   "source": [
    "# deletes with a list comprehension\n",
    "splits = splits_a\n",
    "deletes = [L + R[1:] for L, R in splits if R]\n",
    "\n",
    "print(deletes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Edit Distance\n",
    "Minimium edit distance has wide variety of applications such as implementing spelling correction, document similarity, machine translation, DNA sequencing. It  can be used to evaluate similarity between 2 strings. Given two strings, the minimum distance is the lowest number of operation needed to transform one string into another. `Total edit cost is the sum of cost for edit that were made`. \n",
    "* Insert Cost -> 1\n",
    "* Delete Cost -> 1\n",
    "* Replace Cost -> 2\n",
    "<br/>\n",
    "Tabular approach allows to speed up the enumeration of all possible strings and edits. When computing the minimum distance, we should start with a source word and transform it into the target word. Consider example <br/> <br/>\n",
    "\n",
    "Stay (source word) -> Play (target word) <br/> <br/>\n",
    "\n",
    "From going to `pl` to empty string `''`, minimum edit will be 2 because it involves deleting 2 items `'p'` and `'l'`. Similarly from going to `p -> s` have 2 cost operation as it involve insertion and deletion. The method used is called `Levenshtein distance`. It is the minimum number of single-character edits required to change one string into the other. <br/>\n",
    "\n",
    "`Dynammic Programming`, intuitively, means that solving the smalled sub-problem first and then resuing that result to solve next biggest sub-problem, saving that result and reusing it again,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging and Hidden Markov Models\n",
    "`Part of speech tagging (POS)`is fundamental task in NLP that involves assiging a specific grammatical category to each word in a give text. The primary goal is to analysze and undertand grammatical structure of a sentence. We will be using short representation called `tags` to represent these categories. Consider the following example:\n",
    "\n",
    "`The quick brown fox jumps over the lazy dog`\n",
    "\n",
    "A POS Tagger would analyze this sentence and assign parts of speech to each word resulting in\n",
    "* \"The\" -> Determiner\n",
    "* \"quick\" -> Adjective\n",
    "* \"brown\" -> Adjective\n",
    "* \"fox\" -> Noun\n",
    "* \"jumps\" -> Verb\n",
    "* \"over\" -> Preposition\n",
    "* \"the\" -> Determiner\n",
    "* \"lazy\" -> Adjective\n",
    "* \"dog\" -> Noun "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a vocabulary (Part of Lab)\n",
    "\n",
    "Now that you understand how the dataset is structured, you will create a vocabulary out of it. A vocabulary is made up of every word that appeared at least 2 times in the dataset. \n",
    "For this, follow these steps:\n",
    "- Get only the words from the dataset\n",
    "- Use a defaultdict to count the number of times each word appears\n",
    "- Filter the dict to only include words that appeared at least 2 times\n",
    "- Create a list out of the filtered dict\n",
    "- Sort the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_unk(word):\n",
    "    \"\"\"\n",
    "    Assign tokens to unknown words\n",
    "    \"\"\"\n",
    "\n",
    "    # Punctuation characters\n",
    "    # Try printing them out in a new cell!\n",
    "    punct = set(string.punctuation)\n",
    "\n",
    "    # Suffixes\n",
    "    noun_suffix = [\n",
    "        \"action\",\n",
    "        \"age\",\n",
    "        \"ance\",\n",
    "        \"cy\",\n",
    "        \"dom\",\n",
    "        \"ee\",\n",
    "        \"ence\",\n",
    "        \"er\",\n",
    "        \"hood\",\n",
    "        \"ion\",\n",
    "        \"ism\",\n",
    "        \"ist\",\n",
    "        \"ity\",\n",
    "        \"ling\",\n",
    "        \"ment\",\n",
    "        \"ness\",\n",
    "        \"or\",\n",
    "        \"ry\",\n",
    "        \"scape\",\n",
    "        \"ship\",\n",
    "        \"ty\",\n",
    "    ]\n",
    "    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "    adj_suffix = [\n",
    "        \"able\",\n",
    "        \"ese\",\n",
    "        \"ful\",\n",
    "        \"i\",\n",
    "        \"ian\",\n",
    "        \"ible\",\n",
    "        \"ic\",\n",
    "        \"ish\",\n",
    "        \"ive\",\n",
    "        \"less\",\n",
    "        \"ly\",\n",
    "        \"ous\",\n",
    "    ]\n",
    "    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "    # Loop the characters in the word, check if any is a digit\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return \"--unk_digit--\"\n",
    "\n",
    "    # Loop the characters in the word, check if any is a punctuation character\n",
    "    elif any(char in punct for char in word):\n",
    "        return \"--unk_punct--\"\n",
    "\n",
    "    # Loop the characters in the word, check if any is an upper case character\n",
    "    elif any(char.isupper() for char in word):\n",
    "        return \"--unk_upper--\"\n",
    "\n",
    "    # Check if word ends with any noun suffix\n",
    "    elif any(word.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "\n",
    "    # Check if word ends with any verb suffix\n",
    "    elif any(word.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "\n",
    "    # Check if word ends with any adjective suffix\n",
    "    elif any(word.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "\n",
    "    # Check if word ends with any adverb suffix\n",
    "    elif any(word.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "\n",
    "    # If none of the previous criteria is met, return plain unknown\n",
    "    return \"--unk--\"\n",
    "\n",
    "\n",
    "def get_word_tag(line, vocab):\n",
    "    # If line is empty return placeholders for word and tag\n",
    "    if not line.split():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "    else:\n",
    "        # Split line to separate word and tag\n",
    "        word, tag = line.split()\n",
    "        # Check if word is not in vocabulary\n",
    "        if word not in vocab:\n",
    "            # Handle unknown word\n",
    "            tag = assign_unk(word)\n",
    "    return word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('--n--', '--s--')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag(\"\\n\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain\n",
    "Markov chains is a type of stochastic model that describes a sequence of possible events. It is a mathemtical model that describes a sequence of events where probability of each event depends only on state of previous event. In context of POS tagging, a Markov chain are employed in POS tagging to capture the dependencies between consecutive POS tags. <br/>\n",
    "`Markov property` states that the probability of next event only depends on the current event. It helps in keeping model simple by saying all you need to determine next state is the current state <br/>\n",
    "`Transition matrix` is a sort of table to store the states and transition probabilities. It is a N x N matrix where n = number of states\n",
    "* Each row in matrix represents transition probabilities of one state to all other states.\n",
    "    * In simple words, row represents the current state\n",
    "    * Column represents the next state\n",
    "<br/>\n",
    "\n",
    "`For all outgoing transition probabilities of a give state, the sum of these transition probabilities should always be one`\n",
    "\n",
    "`Hidden markov model (HMM) `implies that states are hidden or not directly observable. HMM are used to model the sequence of observable events (words with POST tags) where the underlying states (hidden states) are the POS tags themselves. <br/>\n",
    "The `emission probability` refers to the probability of observing a particular data point given the underlying hidden state. The emission matrix represents the probabilites for the transition of your `n` hidden state to `m` words in corpous <br/> <br/>\n",
    "\n",
    "In order to calculate all the transition probabilities of a Markov Model, count all the occurences of tag pairs in training corpous. Probabilities can be calculated using\n",
    "\n",
    "`P(ti / ti-1) = C (ti-1, ti) / sum(C (tj-1, tj))`\n",
    "\n",
    "In smoothing, we add small `e` value to each valye of the transition matrix\n",
    "\n",
    "$$ P(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha }{C(t_{i-1}) +\\alpha * N}\\tag{3}$$\n",
    "\n",
    "- $N$ is the total number of tags\n",
    "- $C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in `transition_counts` dictionary.\n",
    "- $C(t_{i-1})$ is the count of the previous POS in the `tag_counts` dictionary.\n",
    "- $\\alpha$ is a smoothing parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tags for Adverb, Noun and To (the preposition) , respectively\n",
    "tags : List[str] = [\"RB\", \"NN\", \"TO\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition_coimts is dictionary which counts the number of times a particular tag happened next to each other\n",
    "transition_counts : Dict[Set[str], int]= {\n",
    "    (\"NN\", \"NN\"): 16241,\n",
    "    (\"RB\", \"RB\"): 2263,\n",
    "    (\"TO\", \"TO\"): 2,\n",
    "    (\"NN\", \"TO\"): 5256,\n",
    "    (\"RB\", \"TO\"): 855,\n",
    "    (\"TO\", \"NN\"): 734,\n",
    "    (\"NN\", \"RB\"): 2431,\n",
    "    (\"RB\", \"NN\"): 358,\n",
    "    (\"TO\", \"RB\"): 200,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a matrix that includes these frequencies\n",
    "\n",
    "num_tags : int = len(tags)\n",
    "# Initialize a 3X3 numpy array with zeros\n",
    "transition_matrix : np.matrix = np.zeros((num_tags, num_tags))\n",
    "\n",
    "# Print matrix\n",
    "transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print shape of the matrix\n",
    "transition_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN', 'RB', 'TO']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sorted version of the tag's list\n",
    "sorted_tags : List[str] = sorted(tags)\n",
    "\n",
    "# Print sorted list\n",
    "sorted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6241e+04, 2.4310e+03, 5.2560e+03],\n",
       "       [3.5800e+02, 2.2630e+03, 8.5500e+02],\n",
       "       [7.3400e+02, 2.0000e+02, 2.0000e+00]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to fill the matrix with correct values, we can use double for loop or itertools.product\n",
    "for i in range(num_tags):\n",
    "    # Loop columns\n",
    "    for j in range(num_tags):\n",
    "        # Define tag pair\n",
    "        tag_tuple = (sorted_tags[i], sorted_tags[j])\n",
    "        # Get frequency from transition_counts dict and assign to (i, j) position in the matrix\n",
    "        transition_matrix[i, j] = transition_counts.get(tag_tuple)\n",
    "\n",
    "# Print matrix\n",
    "transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         NN      RB      TO\n",
      "NN  16241.0  2431.0  5256.0\n",
      "RB    358.0  2263.0   855.0\n",
      "TO    734.0   200.0     2.0\n"
     ]
    }
   ],
   "source": [
    "# Define 'print_matrix' function\n",
    "def print_matrix(matrix):\n",
    "    print(pd.DataFrame(matrix, index=sorted_tags, columns=sorted_tags))\n",
    "\n",
    "\n",
    "# Print the 'transition_matrix' by calling the 'print_matrix' function\n",
    "print_matrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        NN     RB     TO\n",
      "NN  162.41  24.31  52.56\n",
      "RB    3.58  22.63   8.55\n",
      "TO    7.34   2.00   0.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[239.28],\n",
       "       [ 34.76],\n",
       "       [  9.36]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale transition matrix\n",
    "transition_matrix = transition_matrix / 10\n",
    "\n",
    "# Print scaled matrix\n",
    "print_matrix(transition_matrix)\n",
    "\n",
    "rows_sum = transition_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Print sum of rows\n",
    "rows_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          NN        RB        TO\n",
      "NN  0.002837  0.000425  0.000918\n",
      "RB  0.002963  0.018729  0.007076\n",
      "TO  0.083781  0.022829  0.000228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0041792 ],\n",
       "       [0.0287687 ],\n",
       "       [0.10683761]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize transition matrix\n",
    "transition_matrix = transition_matrix / rows_sum\n",
    "\n",
    "# Print normalized matrix\n",
    "print_matrix(transition_matrix)\n",
    "\n",
    "transition_matrix.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Copy transition matrix for for-loop example\n",
    "t_matrix_for = np.copy(transition_matrix)\n",
    "\n",
    "# Copy transition matrix for numpy functions example\n",
    "t_matrix_np = np.copy(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          NN        RB        TO\n",
      "NN  5.480471  0.000425  0.000918\n",
      "RB  0.002963  3.567197  0.007076\n",
      "TO  0.083781  0.022829  2.236674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8184\\629177989.py:3: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  t_matrix_for[i, i] = t_matrix_for[i, i] + math.log(rows_sum[i])\n"
     ]
    }
   ],
   "source": [
    "# Loop values in the diagonal\n",
    "for i in range(num_tags):\n",
    "    t_matrix_for[i, i] = t_matrix_for[i, i] + math.log(rows_sum[i])\n",
    "\n",
    "# Print matrix\n",
    "print_matrix(t_matrix_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save diagonal in a numpy array\n",
    "d = np.diag(t_matrix_np)\n",
    "\n",
    "# Print shape of diagonal\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape diagonal numpy array\n",
    "d = np.reshape(d, (3, 1))\n",
    "\n",
    "# Print shape of diagonal\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           NN         RB        TO\n",
      "NN  16.435740   0.000425  0.000918\n",
      "RB   0.002963  10.664131  0.007076\n",
      "TO   0.083781   0.022829  6.709564\n"
     ]
    }
   ],
   "source": [
    "# Perform the vectorized operation\n",
    "d = d + np.vectorize(math.log)(rows_sum)\n",
    "\n",
    "# Use numpy's 'fill_diagonal' function to update the diagonal\n",
    "np.fill_diagonal(t_matrix_np, d)\n",
    "\n",
    "# Print the matrix\n",
    "print_matrix(t_matrix_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True,  True],\n",
       "       [ True, False,  True],\n",
       "       [ True,  True, False]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for equality\n",
    "t_matrix_for == t_matrix_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm\n",
    "`Viterbi algorithm` is a dynamic programming algorithm that is used to find most probable sequence of POS tags for a give sentence. The algorithm is used to calculate the best path to a node and to find the path to each node with the lowest negative log probability. Three main steps to populate auxiliary matrixes C and D\n",
    "* Intialization\n",
    "    * In this step, the first column of  C and D are populated.\n",
    "    * The first column of C represents probability of the transitions from the state state to first tag (t-1) and word (w1)\n",
    "        * Ci,1 = a1i *bi, index\n",
    "* Forward pass\n",
    "    * All the remaining entries in the two matrices C and D are populated column by column during the forward pass\n",
    "* Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3 Lab-1\n",
    "\n",
    "\\begin{equation*}\n",
    "P(w_n|w_1^{n-1})=\\frac{C(w_1^n)}{C(w_1^{n-1})}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning% makes 'me' happy. i am happy be-cause i am learning! :)\n"
     ]
    }
   ],
   "source": [
    "# change the corpus to lowercase\n",
    "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
    "corpus = corpus.lower()\n",
    "\n",
    "# note that word \"learning\" will now be the same regardless of its position in the sentence\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning makes me happy. i am happy because i am learning! \n"
     ]
    }
   ],
   "source": [
    "# remove special characters\n",
    "corpus = \"learning% makes 'me' happy. i am happy be-cause i am learning! :)\"\n",
    "corpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date parts = ['Sat', 'May', '', '9', '07:33:35', 'CEST', '2020']\n",
      "time parts = ['07', '33', '35']\n"
     ]
    }
   ],
   "source": [
    "# split text by a delimiter to array\n",
    "input_date = \"Sat May  9 07:33:35 CEST 2020\"\n",
    "\n",
    "# get the date parts in array\n",
    "date_parts = input_date.split(\" \")\n",
    "print(f\"date parts = {date_parts}\")\n",
    "\n",
    "# get the time parts in array\n",
    "time_parts = date_parts[4].split(\":\")\n",
    "print(f\"time parts = {time_parts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am happy because i am learning. -> ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize the sentence into an array of words\n",
    "\n",
    "sentence = \"i am happy because i am learning.\"\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "print(f\"{sentence} -> {tokenized_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lengths of the words: \n",
      "[('i', 1), ('am', 2), ('happy', 5), ('because', 7), ('i', 1), ('am', 2), ('learning', 8), ('.', 1)]\n"
     ]
    }
   ],
   "source": [
    "# find length of each word in the tokenized sentence\n",
    "sentence = [\"i\", \"am\", \"happy\", \"because\", \"i\", \"am\", \"learning\", \".\"]\n",
    "word_lengths = [\n",
    "    (word, len(word)) for word in sentence\n",
    "]  # Create a list with the word lengths using a list comprehension\n",
    "print(f\" Lengths of the words: \\n{word_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
      "\n",
      "['i', 'am', 'happy']\n",
      "['am', 'happy', 'because']\n",
      "['happy', 'because', 'i']\n",
      "['because', 'i', 'am']\n",
      "['i', 'am', 'learning']\n",
      "['am', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_trigram(tokenized_sentence):\n",
    "    \"\"\"\n",
    "    Prints all trigrams in the given tokenized sentence.\n",
    "\n",
    "    Args:\n",
    "        tokenized_sentence: The words list.\n",
    "\n",
    "    Returns:\n",
    "        No output\n",
    "    \"\"\"\n",
    "    # note that the last position of i is 3rd to the end\n",
    "    for i in range(len(tokenized_sentence) - 3 + 1):\n",
    "        # the sliding window starts at position i and contains 3 words\n",
    "        trigram = tokenized_sentence[i : i + 3]\n",
    "        print(trigram)\n",
    "\n",
    "\n",
    "tokenized_sentence = [\"i\", \"am\", \"happy\", \"because\", \"i\", \"am\", \"learning\", \".\"]\n",
    "\n",
    "print(f\"List all trigrams of sentence: {tokenized_sentence}\\n\")\n",
    "sentence_to_trigram(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'happy']\n"
     ]
    }
   ],
   "source": [
    "# get trigram prefix from a 4-gram\n",
    "fourgram = [\"i\", \"am\", \"happy\", \"because\"]\n",
    "trigram = fourgram[\n",
    "    0:-1\n",
    "]  # Get the elements from 0, included, up to the last element, not included.\n",
    "print(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2\n",
    "<a name=\"perplexity\"></a>\n",
    "### Perplexity\n",
    "\n",
    "In order to implement the perplexity formula, you'll need to know how to implement m-th order root of a variable.\n",
    "\n",
    "\\begin{equation*}\n",
    "PP(W)=\\sqrt[M]{\\prod_{i=1}^{m}{\\frac{1}{P(w_i|w_{i-1})}}}\n",
    "\\end{equation*}\n",
    "\n",
    "Remember from calculus:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sqrt[M]{\\frac{1}{x}} = x^{-\\frac{1}{M}}\n",
    "\\end{equation*}\n",
    "\n",
    "Here is a code that will help you with the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of n-gram ('i', 'am', 'happy'): 2\n",
      "n-gram ('i', 'am', 'learning') missing\n",
      "n-gram ('i', 'am', 'learning') found\n"
     ]
    }
   ],
   "source": [
    "# manipulate n_gram count dictionary\n",
    "\n",
    "n_gram_counts = {(\"i\", \"am\", \"happy\"): 2, (\"am\", \"happy\", \"because\"): 1}\n",
    "\n",
    "# get count for an n-gram tuple\n",
    "print(f\"count of n-gram {('i', 'am', 'happy')}: {n_gram_counts[('i', 'am', 'happy')]}\")\n",
    "\n",
    "# check if n-gram is present in the dictionary\n",
    "if (\"i\", \"am\", \"learning\") in n_gram_counts:\n",
    "    print(f\"n-gram {('i', 'am', 'learning')} found\")\n",
    "else:\n",
    "    print(f\"n-gram {('i', 'am', 'learning')} missing\")\n",
    "\n",
    "# update the count in the word count dictionary\n",
    "n_gram_counts[(\"i\", \"am\", \"learning\")] = 1\n",
    "if (\"i\", \"am\", \"learning\") in n_gram_counts:\n",
    "    print(f\"n-gram {('i', 'am', 'learning')} found\")\n",
    "else:\n",
    "    print(f\"n-gram {('i', 'am', 'learning')} missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'am', 'happy', 'because')\n"
     ]
    }
   ],
   "source": [
    "# concatenate tuple for prefix and tuple with the last word to create the n_gram\n",
    "prefix = (\"i\", \"am\", \"happy\")\n",
    "word = \"because\"\n",
    "\n",
    "# note here the syntax for creating a tuple for a single word\n",
    "n_gram = prefix + (word,)\n",
    "print(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  happy  because    i   am  learning    .\n",
      "(i, am)             1.0      0.0  0.0  0.0       1.0  0.0\n",
      "(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n",
      "(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n",
      "(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n",
      "(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def single_pass_trigram_count_matrix(corpus):\n",
    "    \"\"\"\n",
    "    Creates the trigram count matrix from the input corpus in a single pass through the corpus.\n",
    "\n",
    "    Args:\n",
    "        corpus: Pre-processed and tokenized corpus.\n",
    "\n",
    "    Returns:\n",
    "        bigrams: list of all bigram prefixes, row index\n",
    "        vocabulary: list of all found words, the column index\n",
    "        count_matrix: pandas dataframe with bigram prefixes as rows,\n",
    "                      vocabulary words as columns\n",
    "                      and the counts of the bigram/word combinations (i.e. trigrams) as values\n",
    "    \"\"\"\n",
    "    bigrams = []\n",
    "    vocabulary = []\n",
    "    count_matrix_dict = defaultdict(dict)\n",
    "\n",
    "    # go through the corpus once with a sliding window\n",
    "    for i in range(len(corpus) - 3 + 1):\n",
    "        # the sliding window starts at position i and contains 3 words\n",
    "        trigram = tuple(corpus[i : i + 3])\n",
    "\n",
    "        bigram = trigram[0:-1]\n",
    "        if not bigram in bigrams:\n",
    "            bigrams.append(bigram)\n",
    "\n",
    "        last_word = trigram[-1]\n",
    "        if not last_word in vocabulary:\n",
    "            vocabulary.append(last_word)\n",
    "\n",
    "        if (bigram, last_word) not in count_matrix_dict:\n",
    "            count_matrix_dict[bigram, last_word] = 0\n",
    "\n",
    "        count_matrix_dict[bigram, last_word] += 1\n",
    "\n",
    "    # convert the count_matrix to np.array to fill in the blanks\n",
    "    count_matrix = np.zeros((len(bigrams), len(vocabulary)))\n",
    "    for trigram_key, trigam_count in count_matrix_dict.items():\n",
    "        count_matrix[\n",
    "            bigrams.index(trigram_key[0]), vocabulary.index(trigram_key[1])\n",
    "        ] = trigam_count\n",
    "\n",
    "    # np.array to pandas dataframe conversion\n",
    "    count_matrix = pd.DataFrame(count_matrix, index=bigrams, columns=vocabulary)\n",
    "    return bigrams, vocabulary, count_matrix\n",
    "\n",
    "\n",
    "corpus = [\"i\", \"am\", \"happy\", \"because\", \"i\", \"am\", \"learning\", \".\"]\n",
    "\n",
    "bigrams, vocabulary, count_matrix = single_pass_trigram_count_matrix(corpus)\n",
    "\n",
    "print(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  happy  because    i   am  learning    .\n",
      "(i, am)             0.5      0.0  0.0  0.0       0.5  0.0\n",
      "(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n",
      "(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n",
      "(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n",
      "(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "# create the probability matrix from the count matrix\n",
    "row_sums = count_matrix.sum(axis=1)\n",
    "# divide each row by its sum\n",
    "prob_matrix = count_matrix.div(row_sums, axis=0)\n",
    "\n",
    "print(prob_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram: ('i', 'am')\n",
      "word: happy\n",
      "trigram_probability: 0.5\n"
     ]
    }
   ],
   "source": [
    "# find the probability of a trigram in the probability matrix\n",
    "trigram = (\"i\", \"am\", \"happy\")\n",
    "\n",
    "# find the prefix bigram\n",
    "bigram = trigram[:-1]\n",
    "print(f\"bigram: {bigram}\")\n",
    "\n",
    "# find the last word of the trigram\n",
    "word = trigram[-1]\n",
    "print(f\"word: {word}\")\n",
    "\n",
    "# we are using the pandas dataframes here, column with vocabulary word comes first, row with the prefix bigram second\n",
    "trigram_probability = prob_matrix[word][bigram]\n",
    "print(f\"trigram_probability: {trigram_probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram: ('i', 'am')\n",
      "word: happy\n",
      "trigram_probability: 0.5\n"
     ]
    }
   ],
   "source": [
    "# find the probability of a trigram in the probability matrix\n",
    "trigram = (\"i\", \"am\", \"happy\")\n",
    "\n",
    "# find the prefix bigram\n",
    "bigram = trigram[:-1]\n",
    "print(f\"bigram: {bigram}\")\n",
    "\n",
    "# find the last word of the trigram\n",
    "word = trigram[-1]\n",
    "print(f\"word: {word}\")\n",
    "\n",
    "# we are using the pandas dataframes here, column with vocabulary word comes first, row with the prefix bigram second\n",
    "trigram_probability = prob_matrix[word][bigram]\n",
    "print(f\"trigram_probability: {trigram_probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in vocabulary starting with prefix: ha\n",
      "\n",
      "happy\n",
      "have\n"
     ]
    }
   ],
   "source": [
    "# lists all words in vocabulary starting with a given prefix\n",
    "vocabulary = [\n",
    "    \"i\",\n",
    "    \"am\",\n",
    "    \"happy\",\n",
    "    \"because\",\n",
    "    \"learning\",\n",
    "    \".\",\n",
    "    \"have\",\n",
    "    \"you\",\n",
    "    \"seen\",\n",
    "    \"it\",\n",
    "    \"?\",\n",
    "]\n",
    "starts_with = \"ha\"\n",
    "\n",
    "print(f\"words in vocabulary starting with prefix: {starts_with}\\n\")\n",
    "for word in vocabulary:\n",
    "    if word.startswith(starts_with):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 80/10/10:\n",
      " train data:[28, 76, 5, 0, 62, 29, 54, 95, 88, 58, 4, 22, 92, 14, 50, 77, 47, 33, 75, 68, 56, 74, 43, 80, 83, 84, 73, 93, 66, 87, 9, 91, 64, 79, 20, 51, 17, 27, 12, 31, 67, 81, 7, 34, 45, 72, 38, 30, 16, 60, 40, 86, 48, 21, 70, 59, 6, 19, 2, 99, 37, 36, 52, 61, 97, 44, 26, 57, 89, 55, 53, 85, 3, 39, 10, 71, 23, 32, 25, 8]\n",
      " validation data:[78, 65, 63, 11, 49, 98, 1, 46, 15, 41]\n",
      " test data:[90, 96, 82, 42, 35, 13, 69, 24, 94, 18]\n",
      "\n",
      "split 98/1/1:\n",
      " train data:[66, 23, 29, 28, 52, 87, 70, 13, 15, 2, 62, 43, 82, 50, 40, 32, 30, 79, 71, 89, 6, 10, 34, 78, 11, 49, 39, 42, 26, 46, 58, 96, 97, 8, 56, 86, 33, 93, 92, 91, 57, 65, 95, 20, 72, 3, 12, 9, 47, 37, 67, 1, 16, 74, 53, 99, 54, 68, 5, 18, 27, 17, 48, 36, 24, 45, 73, 19, 41, 59, 21, 98, 0, 31, 4, 85, 80, 64, 84, 88, 25, 44, 61, 22, 60, 94, 76, 38, 77, 81, 90, 69, 63, 7, 51, 14, 55, 83]\n",
      " validation data:[35]\n",
      " test data:[75]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we only need train and validation %, test is the remainder\n",
    "import random\n",
    "\n",
    "\n",
    "def train_validation_test_split(data, train_percent, validation_percent):\n",
    "    \"\"\"\n",
    "    Splits the input data to  train/validation/test according to the percentage provided\n",
    "\n",
    "    Args:\n",
    "        data: Pre-processed and tokenized corpus, i.e. list of sentences.\n",
    "        train_percent: integer 0-100, defines the portion of input corpus allocated for training\n",
    "        validation_percent: integer 0-100, defines the portion of input corpus allocated for validation\n",
    "\n",
    "        Note: train_percent + validation_percent need to be <=100\n",
    "              the reminder to 100 is allocated for the test set\n",
    "\n",
    "    Returns:\n",
    "        train_data: list of sentences, the training part of the corpus\n",
    "        validation_data: list of sentences, the validation part of the corpus\n",
    "        test_data: list of sentences, the test part of the corpus\n",
    "    \"\"\"\n",
    "    # fixed seed here for reproducibility\n",
    "    random.seed(87)\n",
    "\n",
    "    # reshuffle all input sentences\n",
    "    random.shuffle(data)\n",
    "\n",
    "    train_size = int(len(data) * train_percent / 100)\n",
    "    train_data = data[0:train_size]\n",
    "\n",
    "    validation_size = int(len(data) * validation_percent / 100)\n",
    "    validation_data = data[train_size : train_size + validation_size]\n",
    "\n",
    "    test_data = data[train_size + validation_size :]\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "\n",
    "data = [x for x in range(0, 100)]\n",
    "\n",
    "train_data, validation_data, test_data = train_validation_test_split(data, 80, 10)\n",
    "print(\n",
    "    \"split 80/10/10:\\n\",\n",
    "    f\"train data:{train_data}\\n\",\n",
    "    f\"validation data:{validation_data}\\n\",\n",
    "    f\"test data:{test_data}\\n\",\n",
    ")\n",
    "\n",
    "train_data, validation_data, test_data = train_validation_test_split(data, 98, 1)\n",
    "print(\n",
    "    \"split 98/1/1:\\n\",\n",
    "    f\"train data:{train_data}\\n\",\n",
    "    f\"validation data:{validation_data}\\n\",\n",
    "    f\"test data:{test_data}\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316.22776601683796\n"
     ]
    }
   ],
   "source": [
    "# to calculate the exponent, use the following syntax\n",
    "p = 10 ** (-250)\n",
    "M = 100\n",
    "perplexity = p ** (-1 / M)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the new vocabulary containing 3 most frequent words: ['happy', 'because', 'learning']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary from M most frequent words\n",
    "# use Counter object from the collections library to find M most common words\n",
    "from collections import Counter\n",
    "\n",
    "# the target size of the vocabulary\n",
    "M = 3\n",
    "\n",
    "# pre-calculated word counts\n",
    "# Counter could be used to build this dictionary from the source corpus\n",
    "word_counts = {\"happy\": 5, \"because\": 3, \"i\": 2, \"am\": 2, \"learning\": 3, \".\": 1}\n",
    "\n",
    "vocabulary = Counter(word_counts).most_common(M)\n",
    "\n",
    "# remove the frequencies and leave just the words\n",
    "vocabulary = [w[0] for w in vocabulary]\n",
    "\n",
    "print(f\"the new vocabulary containing {M} most frequent words: {vocabulary}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: ['am', 'i', 'learning']\n",
      "output sentence: ['<UNK>', '<UNK>', 'learning']\n"
     ]
    }
   ],
   "source": [
    "# test if words in the input sentences are in the vocabulary, if OOV, print <UNK>\n",
    "sentence = [\"am\", \"i\", \"learning\"]\n",
    "output_sentence = []\n",
    "print(f\"input sentence: {sentence}\")\n",
    "\n",
    "for w in sentence:\n",
    "    # test if word w is in vocabulary\n",
    "    if w in vocabulary:\n",
    "        output_sentence.append(w)\n",
    "    else:\n",
    "        output_sentence.append(\"<UNK>\")\n",
    "\n",
    "print(f\"output sentence: {output_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because\n",
      "learning\n"
     ]
    }
   ],
   "source": [
    "# iterate through all word counts and print words with given frequency f\n",
    "f = 3\n",
    "\n",
    "word_counts = {\"happy\": 5, \"because\": 3, \"i\": 2, \"am\": 2, \"learning\": 3, \".\": 1}\n",
    "\n",
    "for word, freq in word_counts.items():\n",
    "    if freq == f:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity for the training set: 1.2599210498948732\n",
      "perplexity for the training set with <UNK>: 1.0\n"
     ]
    }
   ],
   "source": [
    "# many <unk> low perplexity\n",
    "training_set = [\"i\", \"am\", \"happy\", \"because\", \"i\", \"am\", \"learning\", \".\"]\n",
    "training_set_unk = [\"i\", \"am\", \"<UNK>\", \"<UNK>\", \"i\", \"am\", \"<UNK>\", \"<UNK>\"]\n",
    "\n",
    "test_set = [\"i\", \"am\", \"learning\"]\n",
    "test_set_unk = [\"i\", \"am\", \"<UNK>\"]\n",
    "\n",
    "M = len(test_set)\n",
    "probability = 1\n",
    "probability_unk = 1\n",
    "\n",
    "# pre-calculated probabilities\n",
    "bigram_probabilities = {\n",
    "    (\"i\", \"am\"): 1.0,\n",
    "    (\"am\", \"happy\"): 0.5,\n",
    "    (\"happy\", \"because\"): 1.0,\n",
    "    (\"because\", \"i\"): 1.0,\n",
    "    (\"am\", \"learning\"): 0.5,\n",
    "    (\"learning\", \".\"): 1.0,\n",
    "}\n",
    "bigram_probabilities_unk = {\n",
    "    (\"i\", \"am\"): 1.0,\n",
    "    (\"am\", \"<UNK>\"): 1.0,\n",
    "    (\"<UNK>\", \"<UNK>\"): 0.5,\n",
    "    (\"<UNK>\", \"i\"): 0.25,\n",
    "}\n",
    "\n",
    "# got through the test set and calculate its bigram probability\n",
    "for i in range(len(test_set) - 2 + 1):\n",
    "    bigram = tuple(test_set[i : i + 2])\n",
    "    probability = probability * bigram_probabilities[bigram]\n",
    "\n",
    "    bigram_unk = tuple(test_set_unk[i : i + 2])\n",
    "    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n",
    "\n",
    "# calculate perplexity for both original test set and test set with <UNK>\n",
    "perplexity = probability ** (-1 / M)\n",
    "perplexity_unk = probability_unk ** (-1 / M)\n",
    "\n",
    "print(f\"perplexity for the training set: {perplexity}\")\n",
    "print(f\"perplexity for the training set with <UNK>: {perplexity_unk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add k-smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability_known_trigram: 0.2\n",
      "probability_unknown_trigram: 0.2\n"
     ]
    }
   ],
   "source": [
    "def add_k_smoothing_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n",
    "    numerator = n_gram_count + k\n",
    "    denominator = n_gram_prefix_count + k * vocabulary_size\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "trigram_probabilities = {(\"i\", \"am\", \"happy\"): 2}\n",
    "bigram_probabilities = {(\"i\", \"am\"): 10}\n",
    "vocabulary_size = 5\n",
    "k = 1\n",
    "\n",
    "probability_known_trigram = add_k_smoothing_probability(\n",
    "    k,\n",
    "    vocabulary_size,\n",
    "    trigram_probabilities[(\"i\", \"am\", \"happy\")],\n",
    "    bigram_probabilities[(\"i\", \"am\")],\n",
    ")\n",
    "\n",
    "probability_unknown_trigram = add_k_smoothing_probability(k, vocabulary_size, 0, 0)\n",
    "\n",
    "print(f\"probability_known_trigram: {probability_known_trigram}\")\n",
    "print(f\"probability_unknown_trigram: {probability_unknown_trigram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "besides the trigram ('are', 'you', 'happy') we also use bigram ('you', 'happy') and unigram (happy)\n",
      "\n",
      "probability for trigram ('are', 'you', 'happy') not found\n",
      "probability for bigram ('you', 'happy') not found\n",
      "probability for unigram happy found\n",
      "\n",
      "probability for trigram ('are', 'you', 'happy') estimated as 0.06400000000000002\n"
     ]
    }
   ],
   "source": [
    "# pre-calculated probabilities of all types of n-grams\n",
    "trigram_probabilities = {(\"i\", \"am\", \"happy\"): 0}\n",
    "bigram_probabilities = {(\"am\", \"happy\"): 0.3}\n",
    "unigram_probabilities = {\"happy\": 0.4}\n",
    "\n",
    "# this is the input trigram we need to estimate\n",
    "trigram = (\"are\", \"you\", \"happy\")\n",
    "\n",
    "# find the last bigram and unigram of the input\n",
    "bigram = trigram[1:3]\n",
    "unigram = trigram[2]\n",
    "print(\n",
    "    f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\"\n",
    ")\n",
    "\n",
    "# 0.4 is used as an example, experimentally found for web-scale corpuses when using the \"stupid\" back-off\n",
    "lambda_factor = 0.4\n",
    "probability_hat_trigram = 0\n",
    "\n",
    "# search for first non-zero probability starting with trigram\n",
    "# to generalize this for any order of n-gram hierarchy,\n",
    "# you could loop through the probability dictionaries instead of if/else cascade\n",
    "if trigram not in trigram_probabilities or trigram_probabilities[trigram] == 0:\n",
    "    print(f\"probability for trigram {trigram} not found\")\n",
    "\n",
    "    if bigram not in bigram_probabilities or bigram_probabilities[bigram] == 0:\n",
    "        print(f\"probability for bigram {bigram} not found\")\n",
    "\n",
    "        if unigram in unigram_probabilities:\n",
    "            print(f\"probability for unigram {unigram} found\\n\")\n",
    "            probability_hat_trigram = (\n",
    "                lambda_factor * lambda_factor * unigram_probabilities[unigram]\n",
    "            )\n",
    "        else:\n",
    "            probability_hat_trigram = 0\n",
    "    else:\n",
    "        probability_hat_trigram = lambda_factor * bigram_probabilities[bigram]\n",
    "else:\n",
    "    probability_hat_trigram = trigram_probabilities[trigram]\n",
    "\n",
    "print(f\"probability for trigram {trigram} estimated as {probability_hat_trigram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "besides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n",
      "\n",
      "estimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n"
     ]
    }
   ],
   "source": [
    "# pre-calculated probabilities of all types of n-grams\n",
    "trigram_probabilities = {(\"i\", \"am\", \"happy\"): 0.15}\n",
    "bigram_probabilities = {(\"am\", \"happy\"): 0.3}\n",
    "unigram_probabilities = {\"happy\": 0.4}\n",
    "\n",
    "# the weights come from optimization on a validation set\n",
    "lambda_1 = 0.8\n",
    "lambda_2 = 0.15\n",
    "lambda_3 = 0.05\n",
    "\n",
    "# this is the input trigram we need to estimate\n",
    "trigram = (\"i\", \"am\", \"happy\")\n",
    "\n",
    "# find the last bigram and unigram of the input\n",
    "bigram = trigram[1:3]\n",
    "unigram = trigram[2]\n",
    "print(\n",
    "    f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\"\n",
    ")\n",
    "\n",
    "# in the production code, you would need to check if the probability n-gram dictionary contains the n-gram\n",
    "probability_hat_trigram = lambda_1 * trigram_probabilities[trigram]\n",
    "+lambda_2 * bigram_probabilities[bigram]\n",
    "+lambda_3 * unigram_probabilities[unigram]\n",
    "\n",
    "print(\n",
    "    f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COurse 4 Lab 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.71320643],\n",
       "       [-4.79248051],\n",
       "       [ 1.33648235],\n",
       "       [ 2.48803883],\n",
       "       [-0.01492988]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a random seed so all random outcomes can be reproduced\n",
    "np.random.seed(10)\n",
    "\n",
    "# Define a 5X1 column vector using numpy\n",
    "z_1 = 10 * np.random.rand(5, 1) - 5\n",
    "\n",
    "# Print the vector\n",
    "z_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of vector and save it in the 'h' variable\n",
    "h = z_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the array or vector. This is the same as applying ReLU to it\n",
    "h[h < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.71320643],\n",
       "       [0.        ],\n",
       "       [1.33648235],\n",
       "       [2.48803883],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the vector after ReLU\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'relu' function that will include the steps previously seen\n",
    "def relu(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [4.50714306],\n",
       "       [2.31993942],\n",
       "       [0.98658484],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a new vector and save it in the 'z' variable\n",
    "z = np.array([[-1.25459881], [4.50714306], [2.31993942], [0.98658484], [-3.4398136]])\n",
    "\n",
    "# Apply ReLU to it\n",
    "relu(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second activation function that you need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n",
    " \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n",
    "\\end{align}\n",
    "\n",
    "To calculate softmax of a vector $\\mathbf{z}$, the $i$-th component of the resulting vector is given by:\n",
    "\n",
    "$$ \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} $$\n",
    "\n",
    "Let's work through an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9. ,  8. , 11. , 10. ,  8.5])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a new vector and save it in the 'z' variable\n",
    "z = np.array([9, 8, 11, 10, 8.5])\n",
    "\n",
    "# Print the vector\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n",
       "        4914.7688403 ])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save exponentials of the values in a new vector\n",
    "e_z = np.exp(z)\n",
    "\n",
    "# Print the vector with the exponential values\n",
    "e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97899.41826492078"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the sum of the exponentials\n",
    "sum_e_z = np.sum(e_z)\n",
    "\n",
    "# Print sum of exponentials\n",
    "sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08276947985173956"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print softmax value of the first element in the original vector\n",
    "e_z[0] / sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'softmax' function that will include the steps previously seen\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z)\n",
    "    return e_z / sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print softmax values for original vector\n",
    "softmax([9, 8, 11, 10, 8.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assert that the sum of the softmax values is equal to 1\n",
    "np.sum(softmax([9, 8, 11, 10, 8.5])) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define V. Remember this was the size of the vocabulary in the previous lecture notebook\n",
    "V = 5\n",
    "\n",
    "# Define vector of length V filled with zeros\n",
    "x_array = np.zeros(V)\n",
    "\n",
    "# Print vector\n",
    "x_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print vector's shape\n",
    "x_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy vector\n",
    "x_column_vector = x_array.copy()\n",
    "\n",
    "# Reshape copy of vector\n",
    "x_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\n",
    "\n",
    "# Print vector\n",
    "x_column_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print vector's shape\n",
    "x_column_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lab 03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
